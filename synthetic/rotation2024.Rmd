---
title: "R Notebook"
output: html_notebook
---



```{r}
rm(list = ls())
library(ggplot2)
library(MASS)
library(splines)
library(latex2exp)
source('..//toSource.R')
library(polynom)
library(SuperLearner)
library("kdensity")

### Models



expit <- function(x){ exp(x)/(1+exp(x)) };

logit <- function(x){ log(x/(1-x)) }

# n <- 4*2000; 
n_vec <- seq(from=500,to=2000,by=100)
nsim <- length(n_vec);

set.seed(1)
cate_hat_mat <- data.frame(matrix(nrow=nsim,ncol=7))
res <- data.frame(matrix(nrow=nsim,ncol=7))
var_mat <- data.frame(matrix(nrow=nsim,ncol=7)) 
colnames(var_mat) = colnames(cate_hat_mat) =colnames(res)= c("plugin","ls_poly","ls_ns","ls_bs", "forster_poly_cross","forster_ns_cross","forster_bs_cross")
var_mat$N = n_vec
res$N = n_vec
cate_hat_mat$N = n_vec

x_new = seq(from = -1, to = 1, by = 0.1) 
n_new = length(x_new)
df_new = data.frame(X=x_new)
for (i in 1:nsim){
  n = n_vec[i]
  n = 10000
  print(n)
  s <- sort(rep(1:2,n/2));
  
  # Measured covariate
  ### TODO 
  
  # ux <- MASS::mvrnorm(n, mu = c(0,0), Sigma = matrix(c(1,0.5,0.5,1),2,2))

  x1 <- runif(n,0,1)
  x2 <- rnorm(n,0,2)
  
  u <- rnorm(n,0,1)
  # Different simulation idea
  
  # IV:
  
  p_z = 0.5
  # Restriction: Independence from U
  z <- rbinom(n,1,prob = p_z)
  
  p_0 <- 0.25
  alpha_z <- log(3)
  alpha_u <-function(u){0.1*(u>0)}
  
  ps <- p_0*exp(alpha_z*z + alpha_u(u))
  true_delta = mean(p_0*exp(alpha_z*1 + alpha_u(u)) - p_0*exp(alpha_u(u)))
  a <- rbinom(n,1,ps);

  #Restrictions: Exclusion restriction and no A,U interaction.
  p_2 <- function(a,u,z){
    #beta_a <- function(u){1*(u>0)}
    ## This ^ is bad for plug in
    beta_a <- function(u){4}

    beta_u <- function(u){0.5*u}
    beta_z <- 1
    return(beta_a(u)*a + beta_u(u) + beta_z*z)
  }
  
  #checking assumption 6:
  
  test_6_1 <- mean(p_2(1,u,1)-p_2(1,u,0)) == mean(p_2(0,u,1)-p_2(0,u,0))
  u_s <- rnorm(n,0,1)
  test_6_2 <- mean(p_2(1,u_s,1)-p_2(1,u_s,0)) == mean(p_2(1,u,1)-p_2(1,u,0))
  test_6_3 <- mean(p_2(0,u_s,1)-p_2(0,u_s,0)) == mean(p_2(0,u,1)-p_2(0,u,0))
  
  
  
  mu0 <- p_2(rep(0,n),u,z);
  mu1 <- p_2(rep(1,n),u,z);
  mu11 <- p_2(rep(1,n),u,rep(1,n));
  mu10 <- p_2(rep(1,n),u,rep(0,n));
  mu_1 <- p_2(a,u,rep(1,n));
  mu_0 <- p_2(a,u,rep(0,n));
  mu_1-mu_0
  mu11-mu10
  ATT <-  sum(a*mu1-a*mu0)/sum(a) 
  
  
  
  
  
  ######## end
  
  
  # outcome
  # under assumption 4.a) no A by U interaction
  error <- rnorm(n,sd=0.2)
  y <- p_2(a,u,z) + error
 
  
  df <- data.frame(X1=x1,X2=x2,Y=y,Z=z,A=a, S= s)
  cov_set = c("X1","X2")
  partition_data <- function(df,cov_set,treat,iv){
    full_data <- df[,cov_set]
    treat_1 <- df[treat,cov_set]
    treat_0 <- df[!treat,cov_set]
    iv_1 <- df[iv,cov_set]
    iv_0 <- df[iv,cov_set]
    treat_1_iv_1 <- df[treat&iv,cov_set]
    treat_1_iv_0 <- df[treat&!iv,cov_set]
    return(list(full = full_data,
                   t1 = treat_1,
                   t0 = treat_0,
                   i1 = iv_1,
                   i0 = iv_0,
                   t1i1 = treat_1_iv_1,
                   t1i0 = treat_1_iv_0))
  }
  parts = partition_data(df,cov_set, a==1,z==1)
  
  # function for cross fitting 1 and 2
  
  
  ##models
  model_pi_1 = SuperLearner(Y = , X = , family = binomial(),SL.library = "SL.glmnet")

  pi_1 = predict(model_pi_1, covars , onlySL = TRUE)[[1]]
  model_pi_0 = SuperLearner(Y = df$A[(z==0)], X = df[(z==0),c("X1","X2")], family = binomial(),SL.library = "SL.glmnet")
  pi_0 = predict(model_pi_0, df[,c("X1","X2")], onlySL = TRUE)[[1]]
  
  
  delta_a = pi_1-pi_0
  
  
  
  model_phi_0= SuperLearner(Y = df$Y[(a == 1)&(z == 0)],
                            X = df[(a == 1)&(z == 0),c("X1","X2")], family = gaussian(),SL.library = "SL.glmnet")
  phi_0 = predict(model_phi_0, df[,c("X1","X2")], onlySL = TRUE)[[1]]

  model_phi_1 = SuperLearner(Y = (df$Y - phi_0)[(a == 1)&(z == 1)],
                            X = df[(a == 1)&(z == 1),c("X1","X2")], family = gaussian(),SL.library = "SL.glmnet")
  phi_1= predict(model_phi_1, df[,c("X1","X2")], onlySL = TRUE)[[1]]

  model_psi_1 = SuperLearner(Y = (df$Y-phi_1*z)[(z == 1)],
                            X = df[(z == 1),c("X1","X2")], family = gaussian(),SL.library = "SL.glmnet")
  psi_1 = predict(model_psi_1, df[,c("X1","X2")], onlySL = TRUE)[[1]]
  
  model_psi_0 = SuperLearner(Y = (df$Y)[(z == 0)],
                            X = df[(z == 0),c("X1","X2")], family = gaussian(),SL.library = "SL.glmnet")
  psi_0 = predict(model_psi_0, df[,c("X1","X2")], onlySL = TRUE)[[1]]
  psi = (psi_1 - psi_0)/delta_a
  
  model_p_Y_0 = SuperLearner(Y = (df$Y - df$A*psi - df$Z*phi_1),
                            X = df[,c("X1","X2")], family = gaussian(),SL.library = "SL.glmnet")
  p_Y_0 = predict(model_p_Y_0, df[,c("X1","X2")], onlySL = TRUE)[[1]]
  # y = Outcome
  # z = IV
  # a = Treatment
  # fz = P(Z=1)(L)
  # faz = P(A=1,Z=1)(L)
  # pi_1 = P(A=1|Z=1)(L)
  ## pi_0 = P(A=1|Z=0)(L)
  # delta_a = pi_1-pi_0
  # phi_1 = E(Y|A=1,Z=1) - E(Y|A=1,Z=0)(L)
  # phi_0 = E(Y|A=1,Z=0)(L)
  # psi = E([G(Y - phi_1) - (1-G)Y]/delta_a)(L)
  # p_Y_0 = E(Y - Apsi - Gphi_1)(L)
  # psi_0 = ATT
  ### p_a = P(A=1)
  EIF = function(y,z,a,fz,faz,pi_1,delta_a,phi_1,phi_0,psi,p_Y_0, psi_0){
    p_a = mean(a)
    nu_1 = pi_1/p_a*(2*z-1)/fz/delta_a
    nu_2 = y - a*psi - z*phi_1 - p_Y_0
    nu_3 = pi_1/p_a*a*(2*z-1)/faz/delta_a
    nu_4 = y - z*phi_1 - phi_0
    nu_5 = (a/p_a)*(psi-psi_0)
    
    return(nu_1*nu_2 - nu_3*nu_4 + nu_5)
  } 
  
  
  
  # independent of L for simplicity: P(Z|L)*P(A|Z,L)
  f_ga = p_z*ps
  
  # E(Y|Z = 1) - E(Y|Z = 0)-[E(Y|A = 1; Z = 1) - E (Y|A = 1; Z = 0)]
  
  
  full_model = SuperLearner(Y = , X = df[,c("Z","A")], family = gaussian(),SL.library = "SL.glmnet")
  z_model = SuperLearner(Y = df$Y, X = , family = binomial(),SL.library = "SL.glmnet")
  m_z1 = predict(z_model,data.frame(Z=rep(1,n)))
  m_z0 = predict(z_model,data.frame(Z=rep(0,n)))
  
  m_z1a1 = predict(full_model,cbind(Z=rep(1,n),A=1), onlySL = TRUE)[[1]]
  m_z0a1 = predict(full_model,cbind(Z=rep(0,n),A=1), onlySL = TRUE)[[1]]
  psi_0 <- mean((m_z1-m_z0 - (m_z1a1-m_z0a1))/delta_a)
  
  
  
  # FW 
  
  po_fitter <- function(df_train, df_test){
  
    m1=
    m0=
    EY1hat <- predict(m1,df_test)
    EY0hat <- predict(m0,df_test)
    # EY1hat <- predict(smooth.spline(df_train[df_train$Z==1,"X"],df_train[df_train$Z==1,"Y"]),df_test$X)$y
    # EY0hat <- predict(smooth.spline(df_train[df_train$Z==0,"X"],df_train[df_train$Z==0,"Y"]),df_test$X)$y
    EA1hat <- predict(glm(A~X, data=df_train[df_train$Z==1,], family=binomial()),data.frame(X=df_test$X),type="response")
    EA0hat <- predict(glm(A~X, data=df_train[df_train$Z==0,], family=binomial()),data.frame(X=df_test$X),type="response")
    
   
    deltahat <- (EA1hat - EA0hat)
    betahat <- (EY1hat-EY0hat)/(EA1hat - EA0hat)
    f_z <- p_z*df_test$Z + (1-p_z)*(1-df_test$Z)
    
    pseudoIV <- (2*df_test$Z - 1)/f_z * (df_test$Y - df_test$A*betahat - EY0hat + EA0hat*betahat)/deltahat + betahat
  }
  
  I_1 <- df[df$S==1,]
  I_2 <- df[df$S==2,]
  I_1$Psuedo = po_fitter(I_2,I_1)
  I_2$Psuedo = po_fitter(I_1,I_2)
  

 

  ## construct estimators
  # forster_poly = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "poly")[[1]]
  # forster_ns = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "ns")[[1]]
  # forster_bs = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "bs")[[1]]

  
  
  CV_DF <- function(X,Y,df_grid=NULL,type,basis_type, KK=5){
    n = length(X) 
    s_test = floor(n/log(n))
    if (is.null(df_grid)){
      df_grid = seq(4,18, by = 2)
      if (basis_type == "bs"){df_grid = seq(50,200, by = 15)}
    }
    l = length(df_grid)
    mse_grid_cv = matrix(NA,l, KK)
    estimator_list = D_list = rep(NA,KK)
    for (k in seq(KK)){
      cat(paste0(k,"/", KK,' '))
      ind_test = sample(1:n, size = s_test) 
      
      g(train_X, train_Y) %=% list(X[-ind_test],  Y[-ind_test])
      g(test_X, test_Y) %=% list(X[ind_test],  Y[ind_test])
      for (i in 1:l){
        df_temp = df_grid[i]
        temp =  series_df(X = train_X, Y=train_Y, x_pred = test_X, df = df_temp ,type = type, basis_type = basis_type)[[1]]
        mse_grid_cv[i,k] = mean((temp - test_Y)^2)
      }
    }
    mean_mses = rowMeans(mse_grid_cv)
    ind = which(mean_mses==min(mean_mses))[[1]]
    return(df_grid[ind])
  }
  
  
  cross_fit <- function(x_test,x_1, x_2, y_1,y_2,type_,basis_, deg){
    
    
    forster_1 = series_df(x_1,y_1,x_test,df=deg, type = type_, basis_type = basis_)[[1]]
    forster_2 = series_df(x_2,y_2,x_test,df=deg, type = type_, basis_type = basis_)[[1]]
    # print(degf_1)
    # print(degf_2)
    return(1/2*(forster_1+forster_2))
  
  }
  deg_poly = CV_DF(df$X,df$Y,type="forster",basis_type="poly")
  forster_poly_cross = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"forster" , "poly",deg_poly)
  
  deg_ns = CV_DF(df$X,df$Y,type="forster",basis_type="ns")
  forster_ns_cross = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo, "forster" , "ns",deg_ns)
  
  deg_bs = CV_DF(df$X,df$Y,type="forster",basis_type="bs")
  forster_bs_cross = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"forster" , "bs",deg_bs)

  # ls_poly = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"ls" , "poly",deg_poly)
  # ls_ns = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"ls" , "bs",deg_ns)
  # ls_bs =cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"ls" , "ns",deg_bs)

  
  
  

  ## save MSEs
  res$plugin[i] <- mean((plugin-cate)^2)
  # res$forster_poly[i] <- mean((forster_poly-cate)^2)
  # res$forster_ns[i] <- mean((forster_ns-cate)^2)
  # res$forster_bs[i] <- mean((forster_bs-cate)^2)
  
  res$forster_poly_cross[i] <- mean((forster_poly_cross-cate)^2)
  res$forster_ns_cross[i] <- mean((forster_ns_cross-cate)^2)
  res$forster_bs_cross[i] <- mean((forster_bs_cross-cate)^2)
  
  
  # res$ls_poly[i] <-mean((ls_poly-cate)^2)
  # res$ls_ns[i] <- mean((ls_ns-cate)^2)
  # res$ls_bs[i] <- mean((ls_bs -cate)^2)
  
  # cate_hat_mat$plugin[i] <- mean(plugin)
  # cate_hat_mat$forster_poly[i] <- mean(forster_poly)
  # cate_hat_mat$forster_ns[i] <- mean(forster_ns)
  # cate_hat_mat$forster_bs[i] <- mean(forster_bs)
  # cate_hat_mat$forster_poly_cross[i] <- mean(forster_poly_cross)
  # cate_hat_mat$forster_ns_cross[i] <- mean(forster_ns_cross)
  # cate_hat_mat$forster_bs_cross[i] <- mean(forster_bs_cross)
  # 
  # cate_hat_mat$ls_poly[i] <-mean(ls_poly)
  # cate_hat_mat$ls_ns[i] <- mean(ls_ns)
  # cate_hat_mat$ls_bs[i] <- mean(ls_bs)
  
  
  
  
}

```



```{r}
#plotting
# res <- res[res$N > 100,]
# s_ = 1
# ggplot(res, aes(x = N))+
#   
#   # geom_line(aes(y = ls_bs, colour = "ls_bs"), linewidth = s_)+
#   geom_line(aes(y = forster_bs_cross, colour = "FW w/ bs"), linewidth = s_)+
#   
#   # geom_line(aes(y = ls_ns, colour = "ls_ns"), linewidth = s_)+
#   geom_line(aes(y = forster_ns_cross, colour = "FW w/ ns"), linewidth = s_)+
#   
#   # geom_line(aes(y = ls_poly, colour = "ls_poly"), linewidth = s_)+
#   geom_line(aes(y = forster_poly_cross, colour = "FW w/ poly"), linewidth = s_)+
#   
#   geom_line(aes(y = plugin, colour = "plugin"), linewidth = s_)+
#   scale_color_manual(values=c("#66A61E","#1B9E77","#E6AB02","#E7298A"))+
#   # scale_color_manual(values=c("#1B9E77", "#D95F02", "#7570B3" ,"#E7298A"))+
#   ggtitle("Mean Squared Error comparison over [-1,1]")+
#   xlab("Number of observations")+
#   ylab("MSE")
```


```{r}

# temp_df = cbind(x_new,forster_bs_cross,forster_poly_cross)
# ggplot(temp_df) + 
#   geom_line(aes(x=x_new,cate,color = "True"), linewidth = s_) +
#   geom_line(aes(x=x_new,y=forster_bs_cross,color = "FW w/ bs"), linewidth = s_) + 
#   geom_line(aes(x=x_new,y=forster_ns_cross,color = "FW w/ ns "), linewidth = s_) +
#   geom_line(aes(x=x_new,y=forster_poly_cross,color="FW w/ polynomial"), linewidth = s_) +
#   geom_line(aes(x=x_new,plugin,color="plugin"), linewidth = s_) +
# 
#   ggtitle("CATE Function Estimates")+
#   xlab("X")+
#   ylab("CATE(X)")+
#   scale_color_manual(name="Function",values=c("#66A61E","#1B9E77","#E6AB02","#E7298A","black"))



```



```{r}

```




