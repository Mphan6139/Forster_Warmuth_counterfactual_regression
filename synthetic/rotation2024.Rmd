---
title: "R Notebook"
output: html_notebook
---



```{r}
rm(list = ls())
library(ggplot2)
library(MASS)
library(splines)
library(latex2exp)
source('..//toSource.R')
library(polynom)


expit <- function(x){ exp(x)/(1+exp(x)) };

logit <- function(x){ log(x/(1-x)) }

# n <- 4*2000; 
n_vec <- seq(from=500,to=2000,by=100)
nsim <- length(n_vec);

set.seed(1)
cate_hat_mat <- data.frame(matrix(nrow=nsim,ncol=7))
res <- data.frame(matrix(nrow=nsim,ncol=7))
var_mat <- data.frame(matrix(nrow=nsim,ncol=7)) 
colnames(var_mat) = colnames(cate_hat_mat) =colnames(res)= c("plugin","ls_poly","ls_ns","ls_bs", "forster_poly_cross","forster_ns_cross","forster_bs_cross")
var_mat$N = n_vec
res$N = n_vec
cate_hat_mat$N = n_vec

x_new = seq(from = -1, to = 1, by = 0.1) 
n_new = length(x_new)
df_new = data.frame(X=x_new)
for (i in 1:nsim){
  n = n_vec[i]
  print(n)
  s <- sort(rep(1:2,n/2));
  
  # Measured covariate
  # x <- runif(n,-1,1)
  # x <- rnorm(n,0,1)
  x <- 2*rbeta(n, 0.6,0.6 )-1
  # ux <- MASS::mvrnorm(n, mu = c(0,0), Sigma = matrix(c(1,0.5,0.5,1),2,2))


  u <- rnorm(n,0,1)
  # Different simulation idea
  # IV:
  p_z = 0.5
  # Restriction: Independence from U
  z <- rbinom(n,1,prob = p_z)
  
  # ps <- 0.2*expit(2*x) + 0.6*z + 0.1*(u>0)
  ps <- (1-z)*(0.2*expit(2*x) + 0.1*(u>0)) + z*(exp(-1*(x^2)))
  range(ps)
  a <- rbinom(n,1,ps);

  #Restrictions: Exclusion restriction and no A,U interaction.
  p_2 <- function(x,a,u){
      # p_x = (x <= -.5)*0.5*(x+2)^2 + (x > -.5 & x<=0)*(x/2+0.875) +
      # (x>0 & x<=.5)*(-5*(x-0.2)^2 +1.075) + (x>.5)*(x+0.125)
      p_x1 = 0.8 + 0.6*x
      p_x0 = 0.8 - 0.2*x + 5*x^2 + 10*x^3

      
      # p_x = (x <= -.5)*0.5 + (x > -.5 & x<=0)*1 +
      # (x>0 & x<=.5)*-0.5 + (x>.5)*2

      # f_a = 0.6
      # f_a = (0.8 - 0.6*x)
      # f_a = (0.8 - 0.6*x + 0.8*x^3)
      # f_a = 1*(-0.5<x)*(x<0.5)
      
    return((1-a)*p_x0 + a*p_x1 + 0.5*u) 
  }
  
  
  mu0 <- p_2(x_new,rep(0,n_new),0);
  mu1 <- p_2(x_new,rep(1,n_new),0);
  
  cate <- mu1-mu0
  
  tau <- mean(cate)
  # outcome
  # under assumption 4.a) no A by U interaction
  error <- rnorm(n,sd=0.2)
  y <- p_2(x,a,u) + error

  
  df <- data.frame(X=x,Y=y,Z=z,A=a, S= s)
  df = df[order(df$X),]
  # function for cross fitting 1 and 2
  
  
  ## plugin  
  
  I_1 <- df[df$S==1,]
  I_2 <- df[df$S==2,]
  
  m1=lm(Y~X,I_1[I_1$Z==1,])
  m0=lm(Y~X,I_1[I_1$Z==0,])
  EY1hat <- predict(m1,data.frame(X=x_new))
  EY0hat <- predict(m0,data.frame(X=x_new))
  # EY1hat <- predict(smooth.spline(I_1[I_1$Z==1,"X"],I_1[I_1$Z==1,"Y"]),x_new)$y
  # EY0hat <- predict(smooth.spline(I_1[I_1$Z==0,"X"],I_1[I_1$Z==0,"Y"]),x_new)$y
  EA1hat <- predict(glm(A~X, data=I_1[I_1$Z==1,], family=binomial()),data.frame(X=x_new),type="response")
  EA0hat <- predict(glm(A~X, data=I_1[I_1$Z==0,], family=binomial()),data.frame(X=x_new),type="response")
  
  plugin <- (EY1hat - EY0hat)/(EA1hat - EA0hat)
  
  
  
  # FW 
  
  po_fitter <- function(df_train, df_test){
  
    m1=lm(Y~X,df_train[df_train$Z==1,])
    m0=lm(Y~X,df_train[df_train$Z==0,])
    EY1hat <- predict(m1,df_test)
    EY0hat <- predict(m0,df_test)
    # EY1hat <- predict(smooth.spline(df_train[df_train$Z==1,"X"],df_train[df_train$Z==1,"Y"]),df_test$X)$y
    # EY0hat <- predict(smooth.spline(df_train[df_train$Z==0,"X"],df_train[df_train$Z==0,"Y"]),df_test$X)$y
    EA1hat <- predict(glm(A~X, data=df_train[df_train$Z==1,], family=binomial()),data.frame(X=df_test$X),type="response")
    EA0hat <- predict(glm(A~X, data=df_train[df_train$Z==0,], family=binomial()),data.frame(X=df_test$X),type="response")
    
   
    deltahat <- (EA1hat - EA0hat)
    betahat <- (EY1hat-EY0hat)/(EA1hat - EA0hat)
    f_z <- p_z*df_test$Z + (1-p_z)*(1-df_test$Z)
    
    pseudoIV <- (2*df_test$Z - 1)/f_z * (df_test$Y - df_test$A*betahat - EY0hat + EA0hat*betahat)/deltahat + betahat
  }
  
  I_1 <- df[df$S==1,]
  I_2 <- df[df$S==2,]
  I_1$Psuedo = po_fitter(I_2,I_1)
  I_2$Psuedo = po_fitter(I_1,I_2)
  

 

  ## construct estimators
  # forster_poly = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "poly")[[1]]
  # forster_ns = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "ns")[[1]]
  # forster_bs = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "bs")[[1]]

  
  
  CV_DF <- function(X,Y,df_grid=NULL,type,basis_type, KK=5){
    n = length(X) 
    s_test = floor(n/log(n))
    if (is.null(df_grid)){
      df_grid = seq(4,18, by = 2)
      if (basis_type == "bs"){df_grid = seq(50,200, by = 15)}
    }
    l = length(df_grid)
    mse_grid_cv = matrix(NA,l, KK)
    estimator_list = D_list = rep(NA,KK)
    for (k in seq(KK)){
      cat(paste0(k,"/", KK,' '))
      ind_test = sample(1:n, size = s_test) 
      
      g(train_X, train_Y) %=% list(X[-ind_test],  Y[-ind_test])
      g(test_X, test_Y) %=% list(X[ind_test],  Y[ind_test])
      for (i in 1:l){
        df_temp = df_grid[i]
        temp =  series_df(X = train_X, Y=train_Y, x_pred = test_X, df = df_temp ,type = type, basis_type = basis_type)[[1]]
        mse_grid_cv[i,k] = mean((temp - test_Y)^2)
      }
    }
    mean_mses = rowMeans(mse_grid_cv)
    ind = which(mean_mses==min(mean_mses))[[1]]
    return(df_grid[ind])
  }
  
  
  cross_fit <- function(x_test,x_1, x_2, y_1,y_2,type_,basis_, deg){
    
    
    forster_1 = series_df(x_1,y_1,x_test,df=deg, type = type_, basis_type = basis_)[[1]]
    forster_2 = series_df(x_2,y_2,x_test,df=deg, type = type_, basis_type = basis_)[[1]]
    # print(degf_1)
    # print(degf_2)
    return(1/2*(forster_1+forster_2))
  
  }
  deg_poly = CV_DF(df$X,df$Y,type="forster",basis_type="poly")
  forster_poly_cross = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"forster" , "poly",deg_poly)
  
  deg_ns = CV_DF(df$X,df$Y,type="forster",basis_type="ns")
  forster_ns_cross = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo, "forster" , "ns",deg_ns)
  
  deg_bs = CV_DF(df$X,df$Y,type="forster",basis_type="bs")
  forster_bs_cross = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"forster" , "bs",deg_bs)

  # ls_poly = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"ls" , "poly",deg_poly)
  # ls_ns = cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"ls" , "bs",deg_ns)
  # ls_bs =cross_fit(df_new$X,I_1$X, I_2$X, I_1$Psuedo,I_2$Psuedo,"ls" , "ns",deg_bs)

  
  
  

  ## save MSEs
  res$plugin[i] <- mean((plugin-cate)^2)
  # res$forster_poly[i] <- mean((forster_poly-cate)^2)
  # res$forster_ns[i] <- mean((forster_ns-cate)^2)
  # res$forster_bs[i] <- mean((forster_bs-cate)^2)
  
  res$forster_poly_cross[i] <- mean((forster_poly_cross-cate)^2)
  res$forster_ns_cross[i] <- mean((forster_ns_cross-cate)^2)
  res$forster_bs_cross[i] <- mean((forster_bs_cross-cate)^2)
  
  
  # res$ls_poly[i] <-mean((ls_poly-cate)^2)
  # res$ls_ns[i] <- mean((ls_ns-cate)^2)
  # res$ls_bs[i] <- mean((ls_bs -cate)^2)
  
  # cate_hat_mat$plugin[i] <- mean(plugin)
  # cate_hat_mat$forster_poly[i] <- mean(forster_poly)
  # cate_hat_mat$forster_ns[i] <- mean(forster_ns)
  # cate_hat_mat$forster_bs[i] <- mean(forster_bs)
  # cate_hat_mat$forster_poly_cross[i] <- mean(forster_poly_cross)
  # cate_hat_mat$forster_ns_cross[i] <- mean(forster_ns_cross)
  # cate_hat_mat$forster_bs_cross[i] <- mean(forster_bs_cross)
  # 
  # cate_hat_mat$ls_poly[i] <-mean(ls_poly)
  # cate_hat_mat$ls_ns[i] <- mean(ls_ns)
  # cate_hat_mat$ls_bs[i] <- mean(ls_bs)
  
  
  
  
}

```



```{r}
#plotting
# res <- res[res$N > 100,]
s_ = 1
ggplot(res, aes(x = N))+
  
  # geom_line(aes(y = ls_bs, colour = "ls_bs"), linewidth = s_)+
  geom_line(aes(y = forster_bs_cross, colour = "FW w/ bs"), linewidth = s_)+
  
  # geom_line(aes(y = ls_ns, colour = "ls_ns"), linewidth = s_)+
  geom_line(aes(y = forster_ns_cross, colour = "FW w/ ns"), linewidth = s_)+
  
  # geom_line(aes(y = ls_poly, colour = "ls_poly"), linewidth = s_)+
  geom_line(aes(y = forster_poly_cross, colour = "FW w/ poly"), linewidth = s_)+
  
  geom_line(aes(y = plugin, colour = "plugin"), linewidth = s_)+
  scale_color_manual(values=c("#66A61E","#1B9E77","#E6AB02","#E7298A"))+
  # scale_color_manual(values=c("#1B9E77", "#D95F02", "#7570B3" ,"#E7298A"))+
  ggtitle("Mean Squared Error comparison over [-1,1]")+
  xlab("Number of observations")+
  ylab("MSE")
```


```{r}

temp_df = cbind(x_new,forster_bs_cross,forster_poly_cross)
ggplot(temp_df) + 
  geom_line(aes(x=x_new,cate,color = "True"), linewidth = s_) +
  geom_line(aes(x=x_new,y=forster_bs_cross,color = "FW w/ bs"), linewidth = s_) + 
  geom_line(aes(x=x_new,y=forster_ns_cross,color = "FW w/ ns "), linewidth = s_) +
  geom_line(aes(x=x_new,y=forster_poly_cross,color="FW w/ polynomial"), linewidth = s_) +
  geom_line(aes(x=x_new,plugin,color="plugin"), linewidth = s_) +

  ggtitle("CATE Function Estimates")+
  xlab("X")+
  ylab("CATE(X)")+
  scale_color_manual(name="Function",values=c("#66A61E","#1B9E77","#E6AB02","#E7298A","black"))



```



```{r}
# ALT data generation
# 
# x <- runif(n,-1,1)
# beta_x <-  + tanh(x)/delta_x
# 
# #P(A|Z=1)=delta_x + P(A|Z=0)
# delta_x <- tanh(0.5*x)
# alpha_x0 <- expit(x + log(3))
# alpha_x1 <- delta_x + alpha_x0
# range(alpha_x1)
# alpha_x <-
# eta_x <- 
```




