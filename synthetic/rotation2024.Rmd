---
title: "R Notebook"
output: html_notebook
---



```{r}
rm(list = ls())
library(ggplot2)
library(MASS)
library(splines)
library(latex2exp)
source('..//toSource.R')
library(polynom)
library(SuperLearner)
library("kdensity")

### Models



expit <- function(x){ exp(x)/(1+exp(x)) };

logit <- function(x){ log(x/(1-x)) }

# n <- 4*2000; 
n_vec <- seq(from=500,to=2000,by=100)
nsim <- length(n_vec);

set.seed(1)
cate_hat_mat <- data.frame(matrix(nrow=nsim,ncol=7))
res <- data.frame(matrix(nrow=nsim,ncol=7))
var_mat <- data.frame(matrix(nrow=nsim,ncol=7)) 
colnames(var_mat) = colnames(cate_hat_mat) =colnames(res)= c("plugin","ls_poly","ls_ns","ls_bs", "forster_poly_cross","forster_ns_cross","forster_bs_cross")
var_mat$N = n_vec
res$N = n_vec
cate_hat_mat$N = n_vec

# x_new = seq(from = -1, to = 1, by = 0.1) 
# n_new = length(x_new)
# df_new = data.frame(X=x_new)
datagen_N = function(n){
    # Covariates:
    x1 <- runif(n,0,1)
    x2 <- rnorm(n,0,2)
    x <- cbind(x1,x2)
    u <- rnorm(n,0,1)
    
    ### Correlated
    # ux <- MASS::mvrnorm(n, mu = c(0,0), Sigma = matrix(c(1,0.5,0.5,1),2,2))
    ###
    
    # IV:
    # Restriction: Independence from U
    p_z = 0.5
    z <- rbinom(n,1,prob = p_z)
    
    # Treatment:
    ps <- function(z,u){
      p_0 <- 0.25
      alpha_z <- log(3)
      alpha_u <-function(u){0.1*(u>0)}
      return(p_0*exp(alpha_z*z + alpha_u(u)))
    }
    a <- rbinom(n,1,ps(z,u));

    # Outcome
    # Restrictions: Exclusion restriction and no A,U interaction.
    p_2 <- function(Z,U,A,X){
      #beta_a <- function(u){1*(u>0)}
      beta_a <- function(u){4}
  
      beta_u <- function(u){0.5*u}
      beta_z <- 1
      beta_x = as.matrix(c(1,-1),nrow=2)

      return(beta_a(U)*A + beta_u(U) + beta_z*Z + X%*%beta_x)
    }
    error <- rnorm(n,sd=0.2)
    y <- p_2(Z=z,U=u,A=a,X=x) + error
    
    test_6_1 <- mean(p_2(Z=1,U=u,A=1,X=x)-p_2(Z=1,U=u,A=0,X=x)) == mean(p_2(Z=0,U=u,A=1,X=x)-p_2(Z=0,U=u,A=0,X=x))

    u_s <- rnorm(n,0,1)
    test_6_2 <- mean(p_2(Z=1,U=u_s,A=1,X=x)-p_2(Z=1,U=u_s,A=0,X=x)) ==
                mean(p_2(Z=1,U=u,  A=1,X=x)-p_2(Z=1,U=u,  A=0,X=x))
    test_6_3 <- mean(p_2(Z=0,U=u_s,A=1,X=x)-p_2(Z=0,U=u_s,A=0,X=x)) ==
                mean(p_2(Z=0,U=u,  A=1,X=x)-p_2(Z=0,U=u,  A=0,X=x))
    
    if(!(test_6_1&test_6_2&test_6_3)){
        stop("assumptions violated!")
    }
    
    mu1z <- p_2(Z=z,U=u,A=rep(1,n),X=x) + error
    mu0z <- p_2(Z=z,U=u,A=rep(0,n),X=x) + error
    ATT <-  sum(a*mu1z-a*mu0z)/sum(a) 
    df <- data.frame(X1=x1,X2=x2,Y=y,Z=z,A=a,Y1 = mu1z, Y0 = mu0z)

    return(df)
  
}

for (i in 1:nsim){
  n = n_vec[i]
  n = 10000
  print(n)
  
  
 
  
  
  ##models
  nuisance_models = function(y, x, a, z){
    
    model_z = SuperLearner(Y = z, X = data.frame(x), family = binomial(),SL.library = "SL.glmnet")
    model_a = SuperLearner(Y = a, X = data.frame(x,Z=z), family = binomial(),SL.library = "SL.glmnet")
    
    
    model_y_za = SuperLearner(Y = y,
                              X = data.frame(x,Z=z,A=a),
                              family = gaussian(),
                              SL.library = "SL.glmnet")
    model_y_z = SuperLearner(Y = y,
                             X = data.frame(x,Z=z),
                             family = gaussian(),
                             SL.library = "SL.glmnet")
    
    
    

    #for phi_1
    Ey_a1i1 = predict(model_y_za, data.frame(x,Z=1,A=1), onlySL = TRUE)[[1]]
    
    # for psi
    Ey_i1 = predict(model_y_z, data.frame(x,Z=1), onlySL = TRUE)[[1]]
    Ey_i0 = predict(model_y_z, data.frame(x,Z=0), onlySL = TRUE)[[1]]
    # is there supposed to be a z*phi_1 or just phi_1?
    
    #for p_Y_0
    p_Y = predict(model_y_za, data.frame(x,A=a,Z=z), onlySL = TRUE)[[1]]
    
    #for EIF
    p_z = predict(model_z, data.frame(x), onlySL = TRUE)[[1]]
    p_az = predict(model_a, data.frame(x,Z=z), onlySL = TRUE)[[1]]
    
    
    pi_1 = predict(model_a, data.frame(x,Z=1) , onlySL = TRUE)[[1]]
    pi_0 = predict(model_a, data.frame(x,Z=0) , onlySL = TRUE)[[1]]
    phi_0 = predict(model_y_za, data.frame(x,Z=0,A=1), onlySL = TRUE)[[1]]
    
    
    phi_1 = Ey_a1i1-phi_0
    psi = (Ey_i1 - Ey_i0-phi_1)/(pi_1-pi_0)
    p_Y_0 = p_Y - a*psi - z*phi_1
    
    

    
    return(data.frame(list(Pi_1 = pi_1,
                Pi_0 = pi_0,
                Phi_1 = phi_1,
                Phi_0 = phi_0,
                Psi = psi,
                P_Y_0 = p_Y_0,
                P_z = p_z,
                P_az = p_az*p_z)))
  }
  
  
  # y = Outcome
  # z = IV
  # a = Treatment
  # fz = P(Z=1)(L)
  # faz = P(A=1,Z=1)(L)
  # pi_1 = P(A=1|Z=1)(L)
  # pi_0 = P(A=1|Z=0)(L)
  # phi_1 = E(Y|A=1,Z=1) - E(Y|A=1,Z=0)(L)
  # phi_0 = E(Y|A=1,Z=0)(L)
  # psi = E([G(Y - phi_1) - (1-G)Y]/delta_a)(L)
  # p_Y_0 = E(Y - Apsi - Gphi_1)(L)
  # plugin = ATT
  ### p_a = P(A=1)
  
  
  nuisance = nuisance_models(y, x, a, z)
  
  p_a = mean(a)
  pi_1 = nuisance$Pi_1
  pi_0 = nuisance$Pi_0
  p_z = nuisance$P_z
  p_az = nuisance$P_az
  phi_1 = nuisance$Phi_1
  phi_0 = nuisance$Phi_0
  psi = nuisance$Psi
  p_Y_0 = nuisance$P_Y_0
  plugin = mean(a/p_a*psi)

  EIF = function(a,z,p_a,pi_1,pi_0,p_z,p_az,phi_1,phi_0,psi,p_Y_0,plugin){
    #plugin: E(psi|A=1)
    #(fz,faz,pi_1,pi_0,phi_1,phi_0,psi,p_Y_0)
    delta_a = pi_1-pi_0

    nu_1 = pi_1/p_a*(2*z-1)/p_z/delta_a
    nu_2 = y - a*psi - z*phi_1 - p_Y_0
    nu_3 = pi_1/p_a*a*(2*z-1)/p_az/delta_a
    nu_4 = y - z*phi_1 - phi_0
    nu_5 = (a/p_a)*(psi-plugin)
    
    return(nu_1*nu_2 - nu_3*nu_4 + nu_5)
  } 

  eif = EIF(a,z,p_a,pi_1,pi_0,p_z,p_az,phi_1,phi_0,psi,p_Y_0,plugin)
  p_o = eif + plugin
  eif
  
}

```



```{r}
#plotting
# res <- res[res$N > 100,]
# s_ = 1
# ggplot(res, aes(x = N))+
#   
#   # geom_line(aes(y = ls_bs, colour = "ls_bs"), linewidth = s_)+
#   geom_line(aes(y = forster_bs_cross, colour = "FW w/ bs"), linewidth = s_)+
#   
#   # geom_line(aes(y = ls_ns, colour = "ls_ns"), linewidth = s_)+
#   geom_line(aes(y = forster_ns_cross, colour = "FW w/ ns"), linewidth = s_)+
#   
#   # geom_line(aes(y = ls_poly, colour = "ls_poly"), linewidth = s_)+
#   geom_line(aes(y = forster_poly_cross, colour = "FW w/ poly"), linewidth = s_)+
#   
#   geom_line(aes(y = plugin, colour = "plugin"), linewidth = s_)+
#   scale_color_manual(values=c("#66A61E","#1B9E77","#E6AB02","#E7298A"))+
#   # scale_color_manual(values=c("#1B9E77", "#D95F02", "#7570B3" ,"#E7298A"))+
#   ggtitle("Mean Squared Error comparison over [-1,1]")+
#   xlab("Number of observations")+
#   ylab("MSE")
```


```{r}

# temp_df = cbind(x_new,forster_bs_cross,forster_poly_cross)
# ggplot(temp_df) + 
#   geom_line(aes(x=x_new,cate,color = "True"), linewidth = s_) +
#   geom_line(aes(x=x_new,y=forster_bs_cross,color = "FW w/ bs"), linewidth = s_) + 
#   geom_line(aes(x=x_new,y=forster_ns_cross,color = "FW w/ ns "), linewidth = s_) +
#   geom_line(aes(x=x_new,y=forster_poly_cross,color="FW w/ polynomial"), linewidth = s_) +
#   geom_line(aes(x=x_new,plugin,color="plugin"), linewidth = s_) +
# 
#   ggtitle("CATE Function Estimates")+
#   xlab("X")+
#   ylab("CATE(X)")+
#   scale_color_manual(name="Function",values=c("#66A61E","#1B9E77","#E6AB02","#E7298A","black"))



```



```{r}

```




