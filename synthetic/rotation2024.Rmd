---
title: "R Notebook"
output: html_notebook
---



```{r}
rm(list = ls())
library(MASS)
library(splines)
library(latex2exp)
source('..//toSource.R')
library(polynom)



### Making a smooth polynomial through set points that looks smooth but not linear in X.
# x_ <- c(-1,-0.5,0,0.5) 
# y_ <- c(0.5,1,1,0.6)
# poly.calc(x_, y_)


expit <- function(x){ exp(x)/(1+exp(x)) };

logit <- function(x){ log(x/(1-x)) }

# n <- 4*2000; 
n_vec <- seq(from=2000,to=8000,by=500)
nsim <- length(n_vec);

set.seed(1)
cate_hat_mat <- data.frame(matrix(nrow=nsim,ncol=7))
res <- data.frame(matrix(nrow=nsim,ncol=7))
var_mat <- data.frame(matrix(nrow=nsim,ncol=7)) 
colnames(var_mat) = colnames(cate_hat_mat) =colnames(res)= c("plugin","ls_poly","ls_ns","ls_bs", "forster_poly_cross","forster_ns_cross","forster_bs_cross")
var_mat$N = n_vec
res$N = n_vec
cate_hat_mat$N = n_vec

for (i in 1:nsim){
  n = n_vec[i]
  s <- sort(rep(1:2,n/2));
  
  
  # Measured covariate
  x <- runif(n,-1,1)
  # unmeasured confounder
  u <- rbinom(n,1,prob=0.5)
  # IV:
  # Restriction: Independence from U
  z <- rbinom(n,1,prob = 0.5)
  # treatment variable: 
  # Restrictions: IV relevance
  
  ps <- expit(2+2*x)*(0.2 + 0.6*z) + 0.1*(u)
  range(ps)
  
  a <- rbinom(n,1,ps);

  
  #Restrictions: Exclusion restriction and no A,U interaction.
  
  p_2 <- function(x,a,u){
      p_x = (x <= -.5)*0.5*(x+2)^2 + (x > -.5 & x<0)*(x/2+0.875) +
      (x>0 & x<.5)*(-5*(x-0.2)^2 +1.075) + (x>.5)*(x+0.125)
      range(p_x)
      # f_a = (x <= -.5)*0.5*(x+2)^2 + (x > -.5 & x<0)*(0.5*x^2) +
      # (x>0)*(-(x-0.5)^2 + 0.5)
      f_a = a*0.3*(0.8 - 0.6*x + 0.9*x^3)
    return(p_x + f_a + 0.1*(2*u-1)) 
  }
 
  
  mu0 <- p_2(x,rep(0,n),u);
  mu1 <- p_2(x,rep(1,n),u);
  cate <- mu1-mu0
  tau <- mean(cate)
  # outcome
  # under assumption 4.a) no A by U interaction
  error <- rnorm(n,sd=0.1)
  y <- p_2(x,a,u) + error

  
  df <- data.frame(X=x,Y=y,Z=z,A=a, S= s)
  df = df[order(df$X, decreasing = TRUE),]
  # function for cross fitting 1 and 2
  
  
 
  I_1 <- df[df$S==1,]
  I_1=I_1[order(I_1$X, decreasing = TRUE),]
  I_2 <- df[df$S==2,]
  I_2=I_2[order(I_2$X, decreasing = TRUE),]

  po_fitter <- function(sub_df){
    ## estimate nuisance functions
    # want to emphasize ability to use non-parametric nuissance functions  

    EA1hat = 0.8*expit(2 + sub_df$X)
    EA0hat = 0.8*expit(sub_df$X)
    
    # use true DGP model
    model_A1 = glm(A ~ ns(X),data = sub_df[sub_df$Z==1,],family = binomial())
    model_A0 = glm(A ~ ns(X),data = sub_df[sub_df$Z==0,],family = binomial())



    EA1hat <- predict(model_A1,newdata = sub_df,type = "response")
    EA0hat <- predict(model_A0,newdata = sub_df,type = "response")
    EY1hat <- predict(smooth.spline(sub_df$X[sub_df$Z==1] , sub_df$Y[sub_df$Z==1]),sub_df$X)$y
    EY0hat <- predict(smooth.spline(sub_df$X[sub_df$Z==0] , sub_df$Y[sub_df$Z==0]),sub_df$X)$y
    
    
    
    betahat <- (EY1hat-EY0hat)/(EA1hat - EA0hat)
    deltahat <- (EA1hat - EA0hat)
    #hard coded. Change later
    f_z <- 0.8*sub_df$Z + 0.2*(1-sub_df$Z)
    pseudoIV <- (2*sub_df$Z - 1)/f_z * (sub_df$Y - sub_df$A*betahat - EY0hat + EA0hat*betahat)/deltahat + betahat
    return(pseudoIV)
  }
  I_1$PsuedoO = po_fitter(I_1)
  I_2$PsuedoO = po_fitter(I_2)
  
    
    
    
  
  

 

  ## construct estimators

  # do we need both IV and Covariate? No, just the covariate.
  # forster_poly = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "poly")[[1]]
  # forster_ns = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "ns")[[1]]
  # forster_bs = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "bs")[[1]]
  
cross_fit <- function(df_, I_1_, I_2_,type_,basis_,knots_){
  forster_1 = series_df(I_1_$X,I_1_$PsuedoO,df_$X,df=knots_, type = type_, basis_type = basis_)[[1]]
  
  forster_2 = series_df(I_2_$X,I_2_$PsuedoO,df_$X,df=knots_, type = type_, basis_type = basis_)[[1]]
  
  return(1/2*(forster_1+forster_2))
  
}
  
  forster_poly_cross = cross_fit(df, I_1, I_2, "forster" , "poly", 4)
  forster_ns_cross = cross_fit(df, I_1, I_2, "forster" , "ns", 50)
  forster_bs_cross = cross_fit(df, I_1, I_2, "forster" , "bs", 50)
  
  ls_poly = cross_fit(df, I_1, I_2, "ls" , "poly",4)
  ls_ns = cross_fit(df, I_1, I_2, "ls" , "ns" , 50)
  ls_bs = cross_fit(df, I_1, I_2, "ls" , "bs" , 50)
  
  # 
  m_A1 = glm(A ~ ns(X),data = df[df$Z==1,],family = binomial())
  m_A0 = glm(A ~ ns(X),data = df[df$Z==0,],family = binomial())
  p_EA1hat <- predict(m_A1,newdata = df,type = "response")
  p_EA0hat <- predict(m_A0,newdata = df,type = "response")
  # p_EA1hat <- 0.8*expit(2 + df$X)
  # p_EA0hat <- 0.8*expit(df$X)
  p_EY1hat <- predict(smooth.spline(df$X[df$Z==1] , df$Y[df$Z==1]),df$X)$y
  p_EY0hat <- predict(smooth.spline(df$X[df$Z==0] , df$Y[df$Z==0]),df$X)$y
    
  
  
  plugin <- (p_EY1hat-p_EY0hat)/(p_EA1hat - p_EA0hat)
  
  

  ## save MSEs
  res$plugin[i] <- mean((plugin-cate)^2)
  # res$forster_poly[i] <- mean((forster_poly-cate)^2)
  # res$forster_ns[i] <- mean((forster_ns-cate)^2)
  # res$forster_bs[i] <- mean((forster_bs-cate)^2)
  
  res$forster_poly_cross[i] <- mean((forster_poly_cross-cate)^2)
  res$forster_ns_cross[i] <- mean((forster_ns_cross-cate)^2)
  res$forster_bs_cross[i] <- mean((forster_bs_cross-cate)^2)
  
  
  res$ls_poly[i] <-mean((ls_poly-cate)^2)
  res$ls_ns[i] <- mean((ls_ns-cate)^2)
  res$ls_bs[i] <- mean((ls_bs -cate)^2)
  # 
  cate_hat_mat$plugin[i] <- mean(plugin)
  # cate_hat_mat$forster_poly[i] <- mean(forster_poly)
  # cate_hat_mat$forster_ns[i] <- mean(forster_ns)
  # cate_hat_mat$forster_bs[i] <- mean(forster_bs)
  cate_hat_mat$forster_poly_cross[i] <- mean(forster_poly_cross)
  cate_hat_mat$forster_ns_cross[i] <- mean(forster_ns_cross)
  cate_hat_mat$forster_bs_cross[i] <- mean(forster_bs_cross)
  
  # cate_hat_mat$ls_poly[i] <-mean(ls_poly-tau)
  # cate_hat_mat$ls_ns[i] <- mean(ls_ns-tau)
  # cate_hat_mat$ls_bs[i] <- mean(ls_bs -tau)
  
  
  
  
}

```

```{r}
#plotting

s_ = 1
ggplot(res, aes(x = N))+
  
  geom_line(aes(y = ls_bs, colour = "ls_bs"), size = s_)+
  geom_line(aes(y = forster_bs_cross, colour = "forster_bs_cross"), size = s_)+
  
  geom_line(aes(y = ls_ns, colour = "ls_ns"), size = s_)+
  geom_line(aes(y = forster_ns_cross, colour = "forster_ns_cross"), size = s_)+
  
  geom_line(aes(y = ls_poly, colour = "ls_poly"), size = s_)+
  geom_line(aes(y = forster_poly_cross, colour = "forster_poly_cross"), size = s_)+
  
  geom_line(aes(y = plugin, colour = "plugin"), size = s_)+
  scale_color_manual(values=c("#66A61E","#1B9E77","#E6AB02", "#D95F02","#A6761D", "#7570B3", "#E7298A"))+
  # scale_color_manual(values=c("#1B9E77", "#D95F02", "#7570B3" ,"#E7298A"))+
  ggtitle("MSE(N) comparison")+
  xlab("Number of observations:2000-8000")+
  ylab("MSE:1/n[(Y - Y*)^2]")
```



```{r}
# Variance plots
DegF = 4
FW_fit_1 = series_df(I_1$X,I_1$PsuedoO,I_2$X,df=DegF, type = "forster", basis_type = "poly")[[1]]
FW_fit_2 = series_df(I_2$X,I_2$PsuedoO,I_1$X,df=DegF, type = "forster", basis_type = "poly")[[1]]
cate_f <- function(x){0.3*(0.8 - 0.6*x + 0.9*x^3)}
I_2$po_ = FW_fit_1 
ggplot(I_2, aes(X))+
  geom_line(aes(y = po_),colour="blue")+
  geom_line(aes(y=cate_f(X),colour = "red"))


# FW_fit_2 = series_df(I_2$X,I_2$PsuedoO,I_1$X,df=DegF, type = "forster", basis_type = "ns")[[1]]

 

# I_2$std_PsO = diag(FW_fit[[2]])
# I_2$lb = I_2$f_hat - 1.96*I_2$std_PsO
# I_2$ub = I_2$f_hat + 1.96*I_2$std_PsO
# std_bs = diag(series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "bs",std=TRUE)[[2]])
#std_ns = diag(series_df(I_1$X,I_1$PsuedoO,I_2$X,df=4, type = "forster", basis_type = "ns",std=TRUE)[[2]])
# ggplot(df, aes(X))+
#   geom_line(aes(y = cross_fit_CATE),colour="blue") 
  #+ geom_ribbon(aes(ymin = lb, ymax = ub), fill = "grey70",alpha=0.4) 

# I_2[c(0,1,2),c("lb","PseudoO","ub")] 


```
```{r}
forster_ns_cross = cross_fit(df, I_1, I_2, "forster" , "ns", 50)
forster_bs_cross = cross_fit(df, I_1, I_2, "forster" , "bs", 50)
```


```{r}
ggplot() + geom_line(aes(x=df$X,y = forster_ns_cross),colour="blue") 
ggplot() + geom_line(aes(x=df$X,y = forster_bs_cross),colour="blue") 

```

```{r}
# p_EA1hat <- 0.8*expit(2 + df$X)
# p_EA0hat <- 0.8*expit(df$X)
# p_EY1hat <- predict(smooth.spline(df$X[df$Z==1] , df$Y[df$Z==1]),df$X)$y
# p_EY0hat <- predict(smooth.spline(df$X[df$Z==0] , df$Y[df$Z==0]),df$X)$y
# plugin <- (p_EY1hat-p_EY0hat)/(p_EA1hat - p_EA0hat)

EY1hat <- predict(smooth.spline(df$X[df$Z==1] , df$Y[df$Z==1]),df$X)$y
EY0hat <- predict(smooth.spline(df$X[df$Z==0] , df$Y[df$Z==0]),df$X)$y
EA1hat = 0.8*expit(2 + df$X)
EA0hat = 0.8*expit(df$X)
plugin = (EY1hat-EY0hat)/(EA1hat-EA0hat)
plot(x,plugin)
```

